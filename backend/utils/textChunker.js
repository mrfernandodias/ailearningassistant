/**
 * ============================================
 * üìÑ TEXT CHUNKER UTILITY
 * ============================================
 *
 * Divide texto grande em peda√ßos menores (chunks) para processar com IA.
 *
 * Por que chunks?
 * - APIs de IA t√™m limite de tokens (palavras)
 * - Textos grandes perdem contexto
 * - Melhor performance e custo
 *
 * Estrat√©gia:
 * 1. Tenta manter par√°grafos inteiros juntos (melhor contexto)
 * 2. Se par√°grafo √© muito grande, divide por palavras
 * 3. Adiciona overlap (sobreposi√ß√£o) entre chunks para n√£o perder contexto
 *
 * Laravel Equivalent: Paginar resultados, mas para texto
 */

/**
 * Divide texto em chunks para processar com IA
 *
 * @param {string} text - Texto completo extra√≠do do PDF
 * @param {number} chunkSize - Tamanho alvo de cada chunk (em palavras). Default: 500
 * @param {number} overlap - Palavras que se repetem entre chunks (mant√©m contexto). Default: 50
 * @returns {Array<{content: string, chunkIndex: number, pageNumber: number}>}
 *
 * @example
 * const chunks = chunkText(pdfText, 500, 50);
 * // Retorna: [{content: "...", chunkIndex: 0}, {content: "...", chunkIndex: 1}, ...]
 */
export const chunkText = (text, chunkSize = 500, overlap = 50) => {
  /**
   * VALIDA√á√ÉO INICIAL
   * Retorna array vazio se texto for null, undefined ou vazio
   */
  if (!text || text.trim().length === 0) {
    return [];
  }

  /**
   * LIMPEZA DO TEXTO
   * Objetivo: Normalizar espa√ßos e quebras de linha
   */
  const cleanedText = text
    .replace(/\r\n/g, '\n') // Windows (\r\n) ‚Üí Unix (\n)
    .replace(/[^\S\n]+/g, ' ') // ‚úÖ Substitui espa√ßos/tabs (mas N√ÉO \n)
    .replace(/\n /g, '\n') // Remove espa√ßo ap√≥s \n
    .replace(/ \n/g, '\n') // Remove espa√ßo antes de \n
    .trim(); // Remove espa√ßos in√≠cio/fim

  /**
   * DIVIDIR EM PAR√ÅGRAFOS
   * ‚ö†Ô∏è PROBLEMA: Como linha 16 removeu todos \n, isso n√£o funciona como esperado
   * Deveria dividir por \n+, mas texto n√£o tem mais \n
   */
  const paragraphs = cleanedText.split(/\n+/).filter(p => p.trim().length > 0);

  /**
   * VARI√ÅVEIS DE CONTROLE
   * chunks: Array final com todos os peda√ßos
   * currentChunk: Par√°grafos sendo acumulados no chunk atual
   * currentWordCount: Contador de palavras do chunk atual
   * chunkIndex: √çndice do chunk (0, 1, 2...)
   */
  const chunks = [];
  let currentChunk = [];
  let currentWordCount = 0;
  let chunkIndex = 0;

  /**
   * LOOP PRINCIPAL: Processa cada par√°grafo
   */
  for (const paragraph of paragraphs) {
    // Dividir par√°grafo em palavras
    const paragraphWords = paragraph.trim().split(/\s+/);
    const paragraphWordCount = paragraphWords.length;

    /**
     * CASO 1: Par√°grafo MAIOR que chunk size
     * Se um √∫nico par√°grafo tem mais palavras que o limite,
     * precisa dividir esse par√°grafo em v√°rios chunks
     */
    if (paragraphWordCount > chunkSize) {
      // Salvar chunk atual antes de processar par√°grafo grande
      if (currentChunk.length > 0) {
        chunks.push({
          content: currentChunk.join('\n\n'), // Junta par√°grafos com dupla quebra
          chunkIndex: chunkIndex++, // Incrementa √≠ndice
          pageNumber: 0, // Placeholder (n√£o temos info de p√°gina)
        });
        currentChunk = [];
        currentWordCount = 0;
      }

      /**
       * Dividir par√°grafo grande em m√∫ltiplos chunks baseados em palavras
       *
       * Exemplo: Par√°grafo com 1500 palavras, chunkSize=500, overlap=50
       * - Chunk 1: palavras 0-500
       * - Chunk 2: palavras 450-950 (overlap de 50 com anterior)
       * - Chunk 3: palavras 900-1400
       * - Chunk 4: palavras 1350-1500
       */
      for (let i = 0; i < paragraphWords.length; i += chunkSize - overlap) {
        const chunkWords = paragraphWords.slice(i, i + chunkSize);
        chunks.push({
          content: chunkWords.join(' '),
          chunkIndex: chunkIndex++,
          pageNumber: 0,
        });

        // Break se chegou no final
        if (i + chunkSize >= paragraphWords.length) break;
      }
      continue; // Pula para pr√≥ximo par√°grafo
    }

    /**
     * CASO 2: Adicionar par√°grafo normal ao chunk atual
     * Verifica se adicionar esse par√°grafo ultrapassa o limite
     */
    if (currentWordCount + paragraphWordCount > chunkSize && currentChunk.length > 0) {
      // Salvar chunk atual (est√° cheio)
      chunks.push({
        content: currentChunk.join('\n\n'),
        chunkIndex: chunkIndex++,
        pageNumber: 0,
      });

      /**
       * CRIAR OVERLAP (Sobreposi√ß√£o)
       * Pega √∫ltimas N palavras do chunk anterior e coloca no in√≠cio do novo
       * Isso mant√©m contexto entre chunks
       *
       * Laravel: Como paginar mas repetir alguns registros entre p√°ginas
       */
      const prevChunkText = currentChunk.join(' ');
      const prevWords = prevChunkText.split(/\s+/);

      // Pega no m√°ximo 'overlap' palavras do final
      const overlapText = prevWords.slice(-Math.min(overlap, prevWords.length)).join(' ');

      // Novo chunk come√ßa com overlap + par√°grafo atual
      currentChunk = [overlapText, paragraph.trim()];
      currentWordCount = overlapText.split(/\s+/).length + paragraphWordCount;
    } else {
      /**
       * Chunk ainda tem espa√ßo, adicionar par√°grafo
       */
      currentChunk.push(paragraph.trim());
      currentWordCount += paragraphWordCount;
    }
  }

  /**
   * ADICIONAR √öLTIMO CHUNK
   * Ap√≥s loop terminar, pode sobrar um chunk n√£o salvo
   */
  if (currentChunk.length > 0) {
    chunks.push({
      content: currentChunk.join('\n\n'),
      chunkIndex: chunkIndex,
      pageNumber: 0,
    });
  }

  /**
   * FALLBACK (Plano B)
   * Se por algum motivo n√£o criou chunks, divide texto bruto por palavras
   * Isso acontece se:
   * - Texto tem algum formato estranho
   * - Limpeza falhou
   * - N√£o conseguiu dividir em par√°grafos
   */
  if (chunks.length === 0 && cleanedText.length > 0) {
    const allWords = cleanedText.split(/\s+/);

    // Divide em chunks de palavras com overlap
    for (let i = 0; i < allWords.length; i += chunkSize - overlap) {
      const chunkWords = allWords.slice(i, i + chunkSize);
      chunks.push({
        content: chunkWords.join(' '),
        chunkIndex: chunkIndex++,
        pageNumber: 0,
      });
      if (i + chunkSize >= allWords.length) break;
    }
  }

  /**
   * RETORNAR CHUNKS
   * Array de objetos: [{content: "...", chunkIndex: 0, pageNumber: 0}, ...]
   */
  return chunks;
};

/**
 * ============================================
 * üîç FIND RELEVANT CHUNKS
 * ============================================
 *
 * Busca os chunks mais relevantes para uma pergunta do usu√°rio.
 * Usa t√©cnica de keyword matching com pontua√ß√£o.
 *
 * Exemplo de uso:
 * - Usu√°rio pergunta: "O que √© machine learning?"
 * - Fun√ß√£o encontra chunks que mencionam "machine" e "learning"
 * - Retorna os 3 melhores chunks para enviar √† IA
 *
 * Laravel Equivalent: Similar a search/filter com scoring
 */

/**
 * Encontra chunks relevantes baseado em correspond√™ncia de palavras-chave
 *
 * @param {Array<Object>} chunks - Array com todos os chunks do documento
 * @param {string} query - Pergunta do usu√°rio
 * @param {number} maxChunks - M√°ximo de chunks a retornar (default: 3)
 * @returns {Array<Object>} Chunks ordenados por relev√¢ncia
 *
 * @example
 * const relevant = findRelevantChunks(allChunks, "machine learning", 3);
 * // Retorna: [
 * //   {content: "...", score: 8.5, chunkIndex: 5},
 * //   {content: "...", score: 6.2, chunkIndex: 12},
 * //   {content: "...", score: 4.1, chunkIndex: 3}
 * // ]
 */
export const findRelevantChunks = (chunks, query, maxChunks = 3) => {
  /**
   * VALIDA√á√ÉO INICIAL
   * Retorna vazio se n√£o h√° chunks ou pergunta
   */
  if (!chunks || chunks.length === 0 || !query) {
    return [];
  }

  /**
   * STOP WORDS (Palavras Irrelevantes)
   *
   * Palavras muito comuns que n√£o ajudam na busca.
   * Exemplo: "o", "de", "que", "the", "is"
   *
   * Set() = estrutura de dados SEM duplicatas
   * - .has() √© O(1) - acesso instant√¢neo (hash)
   * - Array.includes() √© O(n) - precisa varrer tudo
   *
   * Por isso usamos Set para performance!
   * Laravel: Similar a usar array_flip() para acesso r√°pido
   */
  const stopWords = new Set([
    // Stop words em ingl√™s
    'the',
    'is',
    'at',
    'which',
    'on',
    'a',
    'an',
    'and',
    'or',
    'but',
    'in',
    'with',
    'to',
    'for',
    'of',
    'as',
    'by',
    'this',
    'that',
    'it',
    'are',
    'was',
    'were',
    'been',
    'be',
    'have',
    'has',
    'had',
    'do',
    'does',
    'did',
    'will',
    'would',
    'could',
    'should',
    'may',
    'might',

    // Stop words em portugu√™s
    'o',
    'a',
    'os',
    'as',
    'um',
    'uma',
    'uns',
    'umas',
    'de',
    'da',
    'do',
    'das',
    'dos',
    'em',
    'no',
    'na',
    'nos',
    'nas',
    'por',
    'para',
    'com',
    'sem',
    'sob',
    'sobre',
    'e',
    'ou',
    'mas',
    'pois',
    'que',
    'como',
    'quando',
    'onde',
    '√©',
    's√£o',
    'foi',
    'era',
    'ser',
    'estar',
    'ter',
    'haver',
    'isso',
    'este',
    'esse',
    'aquele',
    'esta',
    'essa',
    'aquela',
    'seu',
    'sua',
    'seus',
    'suas',
    'meu',
    'minha',
    'meus',
    'minhas',
  ]);

  /**
   * EXTRAIR PALAVRAS-CHAVE DA PERGUNTA
   *
   * Processo:
   * 1. Converte para min√∫sculas (case-insensitive)
   * 2. Divide por espa√ßos em branco (\s+)
   * 3. Remove palavras muito curtas (< 3 chars)
   * 4. Remove stop words (palavras irrelevantes)
   *
   * Exemplo:
   * "O que √© machine learning?" ‚Üí ["machine", "learning"]
   * (removeu: "o", "que", "√©" - s√£o stop words)
   */
  const queryWords = query
    .toLowerCase()
    .split(/\s+/)
    .filter(w => w.length > 2 && !stopWords.has(w));

  /**
   * FALLBACK: Se nenhuma palavra relevante
   *
   * Pode acontecer se pergunta s√≥ tem stop words:
   * Exemplo: "O que √© isso?" ‚Üí [] (todas s√£o stop words)
   *
   * Nesse caso, retorna primeiros N chunks sem scoring
   */
  if (queryWords.length === 0) {
    return chunks.slice(0, maxChunks).map(chunk => ({
      content: chunk.content,
      chunkIndex: chunk.chunkIndex,
      pageNumber: chunk.pageNumber,
      _id: chunk._id,
    }));
  }

  /**
   * CALCULAR SCORE DE CADA CHUNK
   *
   * Para cada chunk, calcula pontua√ß√£o baseada em:
   * 1. Quantas vezes cada palavra aparece (exactMatches)
   * 2. Quantas palavras diferentes foram encontradas (uniqueWordsFound)
   * 3. Tamanho do chunk (normaliza√ß√£o)
   * 4. Posi√ß√£o no documento (chunks iniciais ganham b√¥nus)
   */
  const scoredChunks = chunks.map((chunk, index) => {
    const content = chunk.content.toLowerCase();
    const contentWords = content.split(/\s+/).length;
    let score = 0;

    /**
     * PONTUAR CADA PALAVRA DA PERGUNTA
     *
     * Para cada palavra-chave, conta quantas vezes aparece no chunk
     * Usa regex \b${word}\b para match EXATO (palavra inteira)
     *
     * Exemplo:
     * - Busca "learn" no texto "learning machine"
     * - \blearn\b N√ÉO encontra em "learning" (precisa ser palavra completa)
     * - Evita falsos positivos
     *
     * Pontua√ß√£o: cada match vale 3 pontos
     */
    for (const word of queryWords) {
      const exactMatches = (content.match(new RegExp(`\\b${word}\\b`, 'g')) || []).length;
      score += exactMatches * 3;
    }

    /**
     * B√îNUS: M√∫ltiplas palavras encontradas
     *
     * Se chunk tem v√°rias palavras da pergunta, ganha b√¥nus extra
     * Indica maior relev√¢ncia para a pergunta completa
     *
     * Exemplo:
     * Pergunta: "machine learning algorithms"
     * - Chunk A: menciona "machine" e "learning" = 2 palavras = +4 pontos
     * - Chunk B: menciona s√≥ "machine" = 1 palavra = 0 b√¥nus
     */
    const uniqueWordsFound = queryWords.filter(word => content.includes(word)).length;
    if (uniqueWordsFound > 1) {
      score += uniqueWordsFound * 2;
    }

    /**
     * NORMALIZA√á√ÉO POR TAMANHO
     *
     * Divide score pela raiz quadrada do tamanho
     * Evita que chunks gigantes sempre ganhem s√≥ por serem maiores
     *
     * Exemplo:
     * - Chunk pequeno (100 palavras): 3 matches = score 3 / ‚àö100 = 0.3
     * - Chunk grande (1000 palavras): 3 matches = score 3 / ‚àö1000 = 0.095
     *
     * Math.sqrt = raiz quadrada
     */
    const normalizedScore = score / Math.sqrt(contentWords);

    /**
     * B√îNUS DE POSI√á√ÉO
     *
     * Chunks no in√≠cio do documento ganham pequeno b√¥nus
     * Assume que informa√ß√µes importantes tendem a vir primeiro
     *
     * F√≥rmula: 1 - (posi√ß√£o / total) * 0.1
     * - Primeiro chunk: 1 - (0 / 100) * 0.1 = 1.0 (sem penalidade)
     * - √öltimo chunk: 1 - (99 / 100) * 0.1 = 0.901 (pequena penalidade)
     *
     * B√¥nus √© pequeno (max 10%) para n√£o sobrepor relev√¢ncia real
     */
    const positionBonus = 1 - (index / chunks.length) * 0.1;

    /**
     * RETORNAR CHUNK COM METADATA
     *
     * Remove propriedades Mongoose (se vier do banco)
     * Adiciona informa√ß√µes de scoring para debug
     */
    return {
      content: chunk.content,
      chunkIndex: chunk.chunkIndex,
      pageNumber: chunk.pageNumber,
      _id: chunk._id,
      score: normalizedScore * positionBonus, // Score final
      rawScore: score, // Score antes de normalizar (debug)
      matchedWords: uniqueWordsFound, // Quantas palavras foram encontradas
    };
  });

  /**
   * FILTRAR, ORDENAR E RETORNAR TOP N
   *
   * 1. Filtra: Remove chunks com score = 0 (nenhuma palavra encontrada)
   * 2. Ordena por:
   *    - Score (maior primeiro)
   *    - Se empate: mais palavras encontradas
   *    - Se ainda empatar: menor √≠ndice (mais cedo no documento)
   * 3. Slice: Retorna apenas os N melhores (maxChunks)
   *
   * Laravel: Similar a ->filter()->sortByDesc()->take()
   */
  return scoredChunks
    .filter(chunk => chunk.score > 0)
    .sort((a, b) => {
      // Ordena√ß√£o prim√°ria: maior score
      if (b.score !== a.score) {
        return b.score - a.score;
      }
      // Desempate 1: mais palavras encontradas
      if (b.matchedWords !== a.matchedWords) {
        return b.matchedWords - a.matchedWords;
      }
      // Desempate 2: chunk mais cedo no documento
      return a.chunkIndex - b.chunkIndex;
    })
    .slice(0, maxChunks);
};

/**
 * ============================================
 * FLUXO DE EXECU√á√ÉO - EXEMPLO
 * ============================================
 *
 * Input:
 * "Primeiro par√°grafo com 300 palavras...\n\nSegundo par√°grafo com 400 palavras..."
 *
 * Processo:
 * 1. Limpa texto (normaliza espa√ßos)
 * 2. Divide em par√°grafos: ["Primeiro...", "Segundo..."]
 * 3. Acumula par√°grafos at√© atingir 500 palavras
 * 4. Quando ultrapassar, salva chunk e come√ßa novo com overlap
 *
 * Output:
 * [
 *   { content: "Primeiro par√°grafo...", chunkIndex: 0, pageNumber: 0 },
 *   { content: "...overlap + Segundo par√°grafo...", chunkIndex: 1, pageNumber: 0 }
 * ]
 */
